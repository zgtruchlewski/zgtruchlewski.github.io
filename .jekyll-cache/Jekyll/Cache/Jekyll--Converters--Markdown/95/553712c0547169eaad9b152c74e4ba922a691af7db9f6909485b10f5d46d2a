I"Vå<p>Many statistical terms are daunting. They can quickly become confusing. Even scholars themselves do not agree on what some mean. Try asking what a <a href="https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/">p-value is at a conference</a> or, if you want to see academics practice kung-fu, ask them whether one should use fixed or random effects (safety not guaranteed). Many concepts sound alike and statistical learners end up feeling like reading a Russian novel: who is Alexandra Petrovna again? And what is her relation to Peter Alexandrovitch? Sometimes, it is even hard to distinguish statistical concepts from expletives used by <a href="https://en.wikipedia.org/wiki/Captain_Haddock#Expletives">Captain Haddock</a>. Being a non-native speaker compounds these difficulties. For instance, what does ‚Äúsignificant‚Äù mean?</p>

<p>I have not yet come across a statistical book with an accessible glossary. So I hope this simple dictionary (work-in-progress!) will help you (and me) to survive the life statistical. Let me know where I can improve it! Use this dictionary at your own peril.</p>

<!-- "‚ÄúUnfortunately, I have yet to find a single person who can explain what ignorability means in a language spoken by those who need to make this assumption or assess its plausibility in a given problem.‚Äù (The book of Why, p. 281)."

Imai 2010 (focus on NDE - fn 3) vs. Balckwell (focus on CDE)

For controlled direct effect, you can freedom to put values in T and Mediator. Also easier to estimate. See Blackwell. For Natural Direct Effect: need to find a particular value of the mediator.  

total effectc an be nul but mechanism can have effect. Gelato example of Aki. 

‚Äú The applied econometrician is like a farmer who notices that the yield is somewhat higher under trees where birds roost, and he uses this as evidence that bird droppings increase yields. However, when he presents this finding at the annual meeting of the American Ecological Association, another farmer in the audience objects that he used the same data but came up with the conclusion that moderate amounts of shade increase yields. A bright chap in the back of the room then observes that these two hypotheses are indistinguishable, given the available data. He mentions the phrase ‚Äùidentification problem,‚Äù which, though no one knows quite what he means, is said with such authority that it is totally convincing.‚Äù ([Leamer, 1983], p. 31). -->

<!-- https://catalogofbias.org/biases/collider-bias/ -->

<!-- https://sgfin.github.io/2019/06/19/Causal-Inference-Book-Glossary-and-Notes/ -->

<h1 id="alphabetically">Alphabetically</h1>

<ul>
  <li>
    <p><strong><em>Berkson‚Äôs paradox:</em></strong></p>
  </li>
  <li>
    <p><strong><em>Bias-variance trade-off:</em></strong></p>
  </li>
  <li><strong><em>Collider:</em></strong> common effect of two causes. In DAGs where two arrows collide. Thus if you condition on this collider, there can be an association between these two causes even though there is no causal relation between them. This is due to the fact that information circulates from one cause to the effect to the second cause.
    <ul>
      <li>This leads to <em>selection bias</em>: association between A and Y even if A does not cause Y. NB: a common effect is not necessarily a collider: a common effect can be the effect of a collider. Selection bias also arises in this case if we condition on the effect of a collider.</li>
      <li><a href="https://www.youtube.com/watch?v=l_7yIUqWBmE&amp;feature=youtu.be">Example</a>: light switch, electricity and light. If you see light is on, and switch is on, then you automatically learn that there is electricitiy. If there is electricity, and there is no light, you automatically deduce that the switch if off (draw DAG below, link to McElreath).<a href="#DAG">DAG</a></li>
    </ul>
  </li>
  <li>
    <p><strong><em>DAG (directed acyclic graph):</em></strong></p>
  </li>
  <li><strong><em>Fitting:</em></strong> REWRITE, from Sololon Kurz: 	Two contrasting kinds of statistical error:
    <ul>
      <li>overfitting, ‚Äúwhich leads to poor prediction by learning too much from the data‚Äù</li>
      <li>underfitting, ‚Äúwhich leads to poor prediction by learning too little from the data‚Äù (p. 166, emphasis added)</li>
    </ul>
  </li>
  <li><strong><em>Probability:</em></strong> Does probability exist? If yes, what is it? It‚Äôs surprising to learn that there is no agreement on this. Some people like Bruno de Finetti claimed that probability does not exist - at least not objectively. It‚Äôs in the eye of the beholder: many people have different probabilities of an event depending on their state of knowledge. Others defined probability base on notions such as a set of events (Kolmogorov), a long run frequency (Venn), or based on all possible outcomes of an experiment (Gosset).
    <ul>
      <li>Seel also: The Lady Tasting Tea, Ch. 29.</li>
    </ul>
  </li>
  <li><strong><em>Sensitivity analysis:</em></strong> Usual done in several ways: 1/ Show how estimates change as we add controsl. Why this is bad? 2/ Fom Imai et al 2011 p. 774 L: Sensitivity analysis provides one way to do this. The goal of a sensitivity analysis is to quantify the exact de- gree to which the key identification assumption must be violated for a researcher‚Äôs original conclusion to be re- versed. If inference is sensitive, a slight violation of the assumption may lead to substantively different conclu- sions. Although sensitivity analyses are not currently a routine part of statistical practice in political science (but see Blattman 2009, and Imai and Yamamoto 2010), we would argue that they should form an indispensable part of empirical research (Rosenbaum, 2002b).</li>
</ul>

<p>Several schools: Stability of Coefficinet visual or table-based (classic, Traunmeuller? Adolph), Bounds (Leamer, Young, Sala-i-Martin), and unobservable confounders (Blackwell, Oster).</p>

<ul>
  <li><strong><em>Significance:</em></strong> distinction between statistical and substantive
    <ul>
      <li>See also: p-value. [####Significance]</li>
    </ul>
  </li>
</ul>

<p><img src="https://zgtruchlewski.github.io/assets/img/sample/Stargazing_BW_Negative.png" width="426" height="281" /></p>

<ul>
  <li>
    <p><strong><em>sequential ignorability:</em></strong></p>
  </li>
  <li>
    <p><strong><em>sharp bound:</em></strong> partially identification of MAnski, mathematically guaranteed bound of ATE vs. Confidence Interval due to uncertainty of sample.</p>
  </li>
</ul>

<!-- These are the most important concepts we've seen in McElreath's book and in the course. Try to skim this through and see what sticks and what does not. Also, if you come by better definitions, please do send them to me!

For definitions: Gelman blog, @parkertransparency2016, Gelman about [Rubin](http://www.stat.columbia.edu/~gelman/research/published/rubin.pdf) and @rohrercausation2018. See also scan of Peter John in the folder. 

confirmation bias, inflated effect size, P-hacking, preregistration, replication, selective reporting

See Frank Harrel here: http://biostat.mc.vanderbilt.edu/wiki/Main/CourseBios330Concepts

- *ATE, ATT and all that (LATE):*
- *autocorrelation:* 
- *Bayes factor:* ratio of average likelihoods (see box on p. 192) and also see blog post in Zotero
- *Berkson's paradox:*
- *bias-variance trade-off:* (box on p. 174)
- *censoring:* See also truncation. you decide on which data to report (e.g. only weights under 200kg). truncated differs from censoring in sense that no count of observations beyond the truncation point is known. Censoring: the values of observations beyond the truncation point are lost but their noumber is observed.¬†
- *Cohen's d:* See this article: https://www.sciencedirect.com/science/article/pii/S1090513818303908#s0090 and reproduce figure 2a. 
- *collider:* common effect of two causes. In DAGs where two arrows collide. Thus if you condition on this collider, there can be an association between these two causes even though there is no causal relation between them. This is due to the fact that information circulates from one cause to the effect to the second cause. This leads to *selection bias*: association between A and Y even if A does not cause Y. NB: a common effect is not necessarily a collider: a common effect can be the effect of a collider. Selection bias also arises in this case if we condition on the effect of a collider.
- *conditioning:* see chapter 7
- *confidence interval:* confidence interval and its many names and [defintions](http://andrewgelman.com/2016/11/26/reminder-instead-confidence-interval-lets-say-uncertainty-interval/)
- *confounder:*
- *collider:*
- *degrees of freedom:* about in the data and in researchers! FOr researchers, see also multiverse. 
- *ecological inference:* see Jeff Gill [here](http://jeffgill.org/papers/EI_oh.pdf)
- *effect of causes vs. causes of effects:* See [Gelman](http://www.stat.columbia.edu/~gelman/research/unpublished/reversecausal_13oct05.pdf)
- *entropy:* 
- *exchangeability:* assumption talked abount in BDA, expand.
- *garden of forking paths:* See Gelman and Loken. see also researcher degrees of freedom in Simmons, J. P., Nelson, L. D., & Simonsohn, U. 2011. False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 20: 1-8. See also Gelman's blog about the analogy between [poker](http://andrewgelman.com/2018/05/21/garden-forking-paths-poker-analogy/#comments) and the Garden of Forking Paths.
- *HARKing:* Kerr, N. L. 1998. HARKing: Hypothesizing after the results are known. Personality and Social Psychology Review, 2: 196-217.
- *Hawthorne effect:* when subjects are aware they are part of an experiments thereby biasing the randomization and so on. See how Instrumental vars can be better here in Schlotter et al. p. 16.
- *Hamiltonian Monte Carlo (HMC):* Also called Hybrid Monte Carlo. See McElreath's blog post. See also Gibbs sampling and Metropolis algorithm.
- *hazard rate:* probability that duration will end after time *t* given that it lasted until time *t*, used in duration models (aka event history analysis). In other words, the hazard rate is the probability that an individual will experience the event at time *t* while the individual is at risk for experiencing the event. See also: survival function, cumulative hazard function. 
- *identification:* ??? ANother difficult question asked by quantitative people who did not listen to the substantial part of your presentation (see also "What about endogeneity?" and "Did you control for X?" [you should make sure that X is necessary and not a collider]). 
- *ignorability:* See Gelman BDA p. 202. study design is called ignorable with respect to the proposed model when the missing data pattern supplies no information and thus the posterior distribution and the posterior predictive distribution of y_mis are entirely determined by the specification of the data model and the observed values.¬†
- *inference:* talk about stats and hypothesis testing: stats are about drawing inference from samples about a population, contrast with prediction? Also say difference between inference and causal inference
- *instrumental variable:* Define and specify conditions/assumption (exogeneity etc). Bewared that there is an old and new definition in Winship and Morgan. Dunning shows how IV model should be specified after the OLS model (assumption of homogenous partial effects). NB: is IV different in a Rubin POF framework?
- *law of small samples:* From Kahneman THInking Fast and Slow - small samples are more likely to yield extreme results (explain). So when you see a study with a small N you should be skeptical of the results. Explain this more in detail as it is important. 
- *likelihood:* the relative plausibility of an observation (p. 275). Distribution of the data. It's a prior for data. Good definition at 21:00 in lecture W10_18
- *location and scale:* Key parameters of any distribution; that is the mean and the sd.
- *marginal effect:*
- *mediator and mediation analysis:* see ch. 5 on post-treatment bias. When thing between treatment and outcome. See Alan M. Jacob et al 2012 as robustness check (i did not fully understand the mechanism)
- *mnesia:* see ch. 13 and video 16
- *model averaging:* Compare to model stacking. 
- *model stacking:* Compare to model averaging. 
- *moderator:* You want to see whether the main effect of interest changes as a function of another variable. E.g. plant life and life, with water. Also called conditional relation: Need to use interaction effect to see whether moderator amplifyies of reduces mechanisms. See interaction.
- *multiverse:* see [in Zotero](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5122713/) and [here](http://www.stat.columbia.edu/~gelman/research/published/multiverse_published.pdf). 
- *non-centered parametrization:* Ch. 13. 
- *over-dispersion:* See ch. 11 and section 11.3 in particular. Over-dispersion happens when an omitted variable produces additional variance after conditioning on all predictors. In summary, there is more variation than expected. See also under-dispersion.
- *parametric and non-parametric regression:* From reddit, "In a nutshell, parametric regression is: "Here is a model, I think this particular function explains the shape." Whether it's a straight line, a polynomial, some non-linear function. Non-parametric regression is more "Uhh... make it smooth, and let it be wiggly where it needs to be. The function's not important." A typical example of parametric regression is the good ol' linear model, Y = a*X + b. Here, a and b are meaningful parameters. An example of a non-parametric regression is loess (locally weighted polynomials). Sure, you can write down a function that describes loess but it's going to be super complicated and you don't actually care about the coefficients of the basis functions. Another good example is fitting a global polynomial, Y = a1 X + a2 X2 + a3 X3 + ... + b. You might not care so much about all the coefficients, but you've got an explicit model you're fitting, a polynomial. Contrast this with a spline, where you don't really care what's going on as long as it satisfies assumptions about continuity and differentiability. Edit: Ruppert, Wand and Carroll's "Semi-parametric regression between 2003-2007" is a good reference."
- *penalization:* Penalizing more complex models in order to avoid overfitting. 
- *p-hacking:* see also
- *pooling:* Explain James Stein estimator and give original reference. See ch. 12/13 and videos 14-15-16-17. Basically it's regression to the mean. 
- *potential outcomes framework (POF):* Rubin et al. See also and compare to DAG of Judea Pearl.
- *prediction:* Talk about machine learning and contrast with inference: machine learning is about finding generalizable predictive patterns. @jamesintroduction2013, p. 20 Here's the paragraph from the book : An Introduction to Statistical Learning "For example, in a real estate setting, one may seek to relate values of homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth. In this case one might be interested in how the individual input variables affect the prices‚Äîthat is, how much extra will a house be worth if it has a view of the river? This is a inference problem. Alternatively, one may simply be interested in predicting the value of a home given its characteristics: is this house under- or over-valued? This is a prediction problem. " See [this link](https://stats.stackexchange.com/questions/244017/prediction-vs-inference?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) 
- *principal stratification:*
- *prior:* Differentiate between fixed - non-adaptive - priors and adaptive priors; also: regularizing priors can be both
- *propensity score:* i.e. restrict analysis to subset of treated and control units with similar distribution of the covariates. Thus propensity score is the probability as a function of the covariates that a unit receives a treatment.¬†
- *posterior:*
- *regularization:* imposing restrictions on the parameters, to avoid overfitting or unrealistic values of the parameters.
- *researcher degrees of freedom:* see Gelman Loken et al
- *shrinkage:* See ch. 12/13 and videos 14-15-16-17
- *Simpson's paradox:* see box in ch. 10 and lecture [13](https://www.youtube.com/watch?v=rSQ1XMwO_9A&index=13&list=PLDcUM9US4XdM9_N6XUUFrhghGJ4K25bFc) for a good example. Reversal of association is not necessarily good: could be due to confound, or collider! Think about your causal path diagram. You should worry about your Simpson paradox, lot of people make this mistake event in prestigious publications.
- *spurious effect:*
- *stability assumption (SUTVA):*
- *survey:* data collection mechanism whereby... Contrast with experiment, observational study and partial-data patterns. See also stratification and clustering, blocking and selections as well as truncation and censoring. 
- *survival function:* In duration models (aka event history analysis), it's the probability that the duration time will be at least *t*. See also: hazard function, cumulative hazard function. 
- *truncation:* See also censoring. BDA III 8.7 for technical treatment. truncated differs from censoring in sense that no count of observations beyond the truncation point is known. Censoring: the values of observations beyond the truncation point are lost but their number is observed.¬†
- *under-dispersion*: When the data has less dispersion than expected. This can arise when there is for instance *autocorrelation*. This means when an observation is dependent upon past or future values, the observed counts has often less variation. 
- *variance-covariance matrix:* 
- *zero-inflated outcomes:* When the zeros in a distribution come from different distributions, that is: there are different processes at play of why zero may arise (either nothing happened or the process in question failed to get started). Thus we need a mixture model to model the two or more processes at play. See section 11.2 of McElreath's book and his example of monks not producing manuscripts any given day either because they did not finish it or because they are drunk.
 -->

<h1 id="sources">Sources</h1>

<h1 id="code">Code</h1>

<p>Link for the DAG code</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre><span class="n">library</span><span class="p">(</span><span class="n">dagitty</span><span class="p">)</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div>

<p>The figure on statisitical vs. substantive significance can be replicated with the following <code class="language-plaintext highlighter-rouge">R</code> code:</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
</pre></td> --><td class="rouge-code"><pre><span class="c1">### Data </span><span class="w">
</span><span class="n">x1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-.2</span><span class="p">,</span><span class="m">.6</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">
</span><span class="n">y1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="m">.2</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">.1</span><span class="p">)</span><span class="w">
</span><span class="n">x2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1.8</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">
</span><span class="n">y2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="m">1.4</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">.1</span><span class="p">)</span><span class="w">
</span><span class="n">x3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="m">2</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">
</span><span class="n">y3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="m">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">.5</span><span class="p">)</span><span class="w">

</span><span class="c1">### Plot </span><span class="w">

</span><span class="c1"># Zero line</span><span class="w">
</span><span class="n">segments</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">-.2</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"gray60"</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="o">=</span><span class="m">2</span><span class="p">)</span><span class="w">

</span><span class="c1"># Densities</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span><span class="n">y2</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"l"</span><span class="p">,</span><span class="w"> </span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">ylim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">axes</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">,</span><span class="w"> 
	</span><span class="n">xlab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="n">ylab</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span><span class="w"> </span><span class="n">xaxs</span><span class="o">=</span><span class="s2">"i"</span><span class="p">,</span><span class="w"> </span><span class="n">yaxs</span><span class="o">=</span><span class="s2">"i"</span><span class="p">)</span><span class="w">
	</span><span class="c1"># # 95% CI shaded</span><span class="w">
	</span><span class="n">x2p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">1.2</span><span class="p">,</span><span class="m">1.6</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">
	</span><span class="n">y2p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x2p</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="m">1.4</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">.1</span><span class="p">)</span><span class="w">
	</span><span class="n">polygon</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">1.2</span><span class="p">,</span><span class="n">x2p</span><span class="p">,</span><span class="m">1.6</span><span class="p">),</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">y2p</span><span class="p">,</span><span class="m">0</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="n">col.alpha</span><span class="p">(</span><span class="s2">"gray"</span><span class="p">,</span><span class="m">0.3</span><span class="p">),</span><span class="w"> </span><span class="n">border</span><span class="o">=</span><span class="kc">NA</span><span class="p">)</span><span class="w">

</span><span class="n">lines</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">y1</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"l"</span><span class="p">,</span><span class="w"> </span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">yaxt</span><span class="o">=</span><span class="s1">'n'</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
	</span><span class="c1"># # 95% CI shaded</span><span class="w">
	</span><span class="n">x1p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">.4</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">
	</span><span class="n">y1p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x1p</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">.1</span><span class="p">)</span><span class="w">
	</span><span class="n">polygon</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">x1p</span><span class="p">,</span><span class="m">.4</span><span class="p">),</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">y1p</span><span class="p">,</span><span class="m">0</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="n">col.alpha</span><span class="p">(</span><span class="s2">"gray"</span><span class="p">,</span><span class="m">0.3</span><span class="p">),</span><span class="w"> </span><span class="n">border</span><span class="o">=</span><span class="kc">NA</span><span class="p">)</span><span class="w">

</span><span class="n">lines</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span><span class="n">y3</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"l"</span><span class="p">,</span><span class="w"> </span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">yaxt</span><span class="o">=</span><span class="s1">'n'</span><span class="p">,</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="m">1</span><span class="p">)</span><span class="w">
	</span><span class="c1"># # 95% CI shaded</span><span class="w">
	</span><span class="n">x3p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">-.2</span><span class="p">,</span><span class="m">1.8</span><span class="p">,</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span><span class="w">
	</span><span class="n">y3p</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dnorm</span><span class="p">(</span><span class="n">x3p</span><span class="p">,</span><span class="n">mean</span><span class="o">=</span><span class="m">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">sd</span><span class="o">=</span><span class="m">.5</span><span class="p">)</span><span class="w">
	</span><span class="n">polygon</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="m">-.2</span><span class="p">,</span><span class="n">x3p</span><span class="p">,</span><span class="m">1.8</span><span class="p">),</span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="n">y3p</span><span class="p">,</span><span class="m">0</span><span class="p">),</span><span class="n">col</span><span class="o">=</span><span class="n">col.alpha</span><span class="p">(</span><span class="s2">"gray"</span><span class="p">,</span><span class="m">0.3</span><span class="p">),</span><span class="w"> </span><span class="n">border</span><span class="o">=</span><span class="kc">NA</span><span class="p">)</span><span class="w">

</span><span class="n">axis</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">xlim</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> </span><span class="n">at</span><span class="o">=</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="m">-1</span><span class="p">,</span><span class="w"> </span><span class="m">-0.5</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1.5</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">),</span><span class="w"> 
	</span><span class="n">labels</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"-2"</span><span class="p">,</span><span class="w"> </span><span class="s2">"-1"</span><span class="p">,</span><span class="w"> </span><span class="s2">"0"</span><span class="p">,</span><span class="w"> </span><span class="s2">"1"</span><span class="p">,</span><span class="w"> </span><span class="s2">"2"</span><span class="p">,</span><span class="w"> </span><span class="s2">"3"</span><span class="p">,</span><span class="w"> </span><span class="s2">"4"</span><span class="p">))</span><span class="w">

</span><span class="c1"># Add text</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="m">-.775</span><span class="p">,</span><span class="w"> </span><span class="m">4.25</span><span class="p">,</span><span class="w"> </span><span class="s2">"Statistically significant?"</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="m">-.775</span><span class="p">,</span><span class="w"> </span><span class="m">4.5</span><span class="p">,</span><span class="w"> </span><span class="s2">"Substantively significant?"</span><span class="p">)</span><span class="w">

</span><span class="n">text</span><span class="p">(</span><span class="m">.15</span><span class="p">,</span><span class="w"> </span><span class="m">4.25</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yes"</span><span class="p">)</span><span class="w"> 
</span><span class="n">text</span><span class="p">(</span><span class="m">.15</span><span class="p">,</span><span class="w"> </span><span class="m">4.5</span><span class="p">,</span><span class="w"> </span><span class="s2">"No"</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="m">.2</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="s2">"Precision"</span><span class="p">)</span><span class="w">

</span><span class="n">text</span><span class="p">(</span><span class="m">1.35</span><span class="p">,</span><span class="w"> </span><span class="m">4.25</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yes"</span><span class="p">)</span><span class="w"> 
</span><span class="n">text</span><span class="p">(</span><span class="m">1.35</span><span class="p">,</span><span class="w"> </span><span class="m">4.5</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yes"</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="m">1.4</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="s2">"Ooomph &amp;\nprecision"</span><span class="p">)</span><span class="w">

</span><span class="n">text</span><span class="p">(</span><span class="m">0.75</span><span class="p">,</span><span class="w"> </span><span class="m">4.25</span><span class="p">,</span><span class="w"> </span><span class="s2">"No"</span><span class="p">)</span><span class="w"> 
</span><span class="n">text</span><span class="p">(</span><span class="m">0.75</span><span class="p">,</span><span class="w"> </span><span class="m">4.5</span><span class="p">,</span><span class="w"> </span><span class="s2">"Yes"</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="m">0.8</span><span class="p">,</span><span class="w"> </span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="s2">"Ooomph"</span><span class="p">)</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div>
:ET